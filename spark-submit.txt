Why SPARK_EXTRA_CLASSPATH didnâ€™t help

This is the root cause ðŸ‘‡

Your docker-compose:
SPARK_EXTRA_CLASSPATH=/opt/spark/jars/extra-jar/*

Reality:

Spark does NOT automatically use SPARK_EXTRA_CLASSPATH

Spark only understands:

spark.driver.extraClassPath

spark.executor.extraClassPath

--jars

--driver-class-path

So this env var is ignored by spark-submit

----------------------------------------------------------------------------------------------------

To run the code use the above option to send the jar to the JVM Without that you will get the error 
java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver

this is because your spark and container has the jdbc jar file but the JVM where the code run did not have that file 
to fix this use the option given below to send the jar to the JVM while submting the code.
IMP NOTE :- YOU HAVE TO explicitly use this while submiting the code in spark-submit command or create config file which is use by 
spark-submit automatically and no need to write it again and again below option while submiting(WHICH IS
 ðŸŸ¢ OPTION 3 (Docker-native, cleanest): spark-defaults.conf âœ…)

--jars
--driver-class-path
spark.driver.extraClassPath
spark.executor.extraClassPath
spark-defaults.conf

----------------------------------------------------------------------------------------------------------------------

SPARK SUBMIT COMMAND

Spark submit command 

ðŸŸ¢ OPTION 1 (BEST): Fix spark-submit command
Run this instead:
docker exec -it spark-master \
  /opt/spark/bin/spark-submit \
  --jars /opt/spark/jars/extra-jar/mssql-jdbc-13.2.1.jre11.jar \
  /opt/spark-app/job2.py
âœ” Driver class loaded
âœ” Executors get JAR
âœ” Production-safe
________________________________________
ðŸŸ¢ OPTION 2: Use driver + executor classpath explicitly
docker exec -it spark-master \
  /opt/spark/bin/spark-submit \
  --driver-class-path /opt/spark/jars/extra-jar/mssql-jdbc-13.2.1.jre11.jar \
  --conf spark.executor.extraClassPath=/opt/spark/jars/extra-jar/mssql-jdbc-13.2.1.jre11.jar \
  /opt/spark-app/job2.py
________________________________________
ðŸŸ¢ OPTION 3 (Docker-native, cleanest): spark-defaults.conf âœ…
Create file:
./conf/spark-defaults.conf
Add this to file 
spark.driver.extraClassPath /opt/spark/jars/extra-jar/*
spark.executor.extraClassPath /opt/spark/jars/extra-jar/*
Mount it:
volumes:
  - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
Now you can run:
spark-submit /opt/spark-app/job2.py
No flags needed ðŸŽ¯


